---
    title: "Predicting Titanic Survival from Passenger Records: An example of Machine Learning using caret in R."
author: "Iain Harlow"
date: 
output:
    html_document:
    keep_md: yes
pdf_document: default
---
    
    ___
## Synopsis

This is a tutorial in R for machine-learning driven predictions of Titanic Disaster survival likelihood, based on passenger records. This kaggle 'contest' is non-competitive and so I'll go step-by-step through my approach in the hope that you find it useful or interesting!

The aim here is to:

1. Demonstrate the use of R, and in particular the caret package, for machine learning with a variety of different models and approaches.

2. Get to understand the strengths and weaknesses of each machine learning method, both by comparing their performances (and the correlations between their predictions) but also by systematically examining how different settings for each method affect how they learn.

3. Emphasise the importance of feature selection and engineering - we have many machine learning algorithms available, and it's possible to stack and combine many together to fine-tune performance. But ultimately, they can only extract the information we feed them - so curating the input is a huge part of successful machine learning.

### Approach

Glancing through the available data and the event [background](http://en.wikipedia.org/wiki/Sinking_of_the_RMS_Titanic) suggests a few things straight away:

- Certain features in the data are likely to be very strong predictors of survival - class, sex and age in particular.
- But the interaction of these features may also be very important: For example, the survival of male and female passengers may show different relationships with age or class.
- We have a relatively small dataset, and a fairly high number of predictors, so overfitting is something we shoudl definitely be cautious about.

Let's use this dataset to play with some different classification algorithms in R. We can compare their performance to get a sense of how each performs, and perhaps build a final model incorporating more than one (i.e. "stack" the models).

We'll also perform some simple munging or feature engineering to extract as clean and relevant a set of predictors as possible. This dataset is a nice one to demonstrate this, since we have a reasonable intuition about what each of the features actually reflects and how it might affect the model (often this can be more ambiguous).

## Part 1: Benchmarking Performance

First off, let's take a look at the leaderboard to get an idea of benchmark performance. When you download the zip file from the [leaderboard page](https://www.kaggle.com/c/titanic-gettingStarted/leaderboard) you get a list of every entry made *that improved on the submitters previous score*. So the scores here are going to be a biased reflection of how people's models are performing out-of-sample. We'll also mark the 'benchmark' models on here, too:

```{r, cache=TRUE}
setwd("C:/Users/Iain/Google Drive/Github/Blog/Titanic-Kaggle-Tutorial")
suppressWarnings(suppressMessages(library(ggplot2))) # For plotting
improvements <- read.csv("titanic-gettingStarted_public_leaderboard.csv",stringsAsFactors=FALSE)

d<-data.frame(model=c("All Perish","Gender","Random Forest","Gen/Price/Class"),
              score=c(0.62679,0.76555,0.77512,0.77990))

ggplot(improvements, aes(x=Score)) +
    geom_histogram(binwidth=1/200) +
    xlim(0.6, 0.9) +
    ylim(0,3000) +
    ggtitle("Titanic Prediction Accuracies (biased sample of 'best' scores)") + 
    theme(plot.title = element_text(size=20, vjust=1.5)) +
    geom_vline(data=d, 
               mapping=aes(xintercept=score), 
               color="blue") +
    geom_text(data=d, 
              mapping=aes(x=score, y=1100, label=model), 
              size=4, 
              colour="blue",
              angle=90, 
              vjust=-0.1, 
              hjust=0)
```

This gives us a pretty good idea of how well we can expect our model to do. The absolute baseline is provided by the "All Perish" model - that is, just pick the most likely outcome (Survival = 0) and apply it to all passengers. This requires no machine learning other than to estimate which outcome is more likely from the training set.

Clearly, it's possible to do quite a bit better, and get a score in the 0.7 to 0.85 range. A very simple Gender model (split the passengers by gender before applying the most likely outcome - females survive, males do not) scores around 76.5%. Adding in ticket price and class gets the classification accuracy up to 78%, while a random forest on the numerical and factor variables scores similarly at 77.5%. These also coincide with the most obvious peaks in the histogram.

We can safely ignore any scores near zero (probably an error creating the CSV file) or near one (probably just looking up the true outcomes, which are readily available on Wikipedia). In fact, it seems like the best we might hope for would be around 80%, and certainly no higher than 85%. It's worth remembering, too, that with near-unlimited submissions it's quite possible to overfit to the visible part of the test data, so many of those 80%+ models could perform quite porrly on the hidden half of the test data (again, assuming they haven't cheated and looked it up!).

Let's also take a look just at each team's first entry - this will give a less severely biased impression of how well the models perform (and in the opposite direction) - in other words a reflection of how people's simplest/first ideas work out:

```{r}
firsts <- improvements[!duplicated(improvements$TeamName),]

ggplot(firsts, aes(x=Score)) +
    geom_histogram(binwidth=1/200) +
    xlim(0.6, 0.9) +
    ylim(0,1000) +
    ggtitle("Titanic Prediction Accuracies (initial scores only)") + 
    theme(plot.title = element_text(size=20, vjust=1.5)) +
    geom_vline(data=d, 
               mapping=aes(xintercept=score), 
               color="blue")
```

We see the same three peaks as before, but relatively few initial attempts get over 78%. Quite a few score lower than the very simple "Gender" benchmark, which is characteristic of overfitting to the training data.

So overall, it looks like a model scoring 78-80% is probably doing pretty well. If we submit several sufficently distinct models, we're likely to get a few scores above 80% just through chance, but 78-80% is still likely to be the range we'll get for the unseen data.

Finally, since a simple gender split gives 77%, scores below ~75% for more complex models are likely a warning sign that we're overfitting. The random forest benchmark is also quite useful, as random forests tend to be a fairly powerful method for extracting relationships between the features and outcome (if a little hard to interpret and prone to overfitting). This random forest gives us an idea of how much we can extract with a powerful model simply by using the features as they are. In the next section we'll take a look at the features we have available, and assess them one-by-one.

## Part 2: Features

A good approach I think, especially when we have a reasonably limited and interpretable feature set, is to go through them one by one and consider how useful or predictive we'd expect them to be. We'll also have a think about how to manipulate or reformat them to perhaps improve their predictive power, or even discard some at this stage.

#### Pclass
The class of the passenger is likely to be a pretty useful predictor for a couple of reasons. Firstly, it gives us some information about where on the ship they probably were when the collision occured (since this was at night): Upper class cabins were generally higher on the ship, further from the early flooding and closer to the lifeboats. Secondly, survival on the Titanic seems to have been influenced by social as well as physical factors - "women and children first" seems to have applied to a certain extent in allocating lifeboat space, and class may also have had an influence on behaviour. Similarly, it's possible to imagine interactions with other factors such as age or gender. There are three levels of class, so we have two choices - encode it as a numeric variable (i.e. assume 2nd class is equally 'different' to 1st and 3rd) or encode it as two dummy variables. The latter uses an extra degree of freedom to allow 2nd class to be more similar to one of the other two, for example if 1st and 2nd class had similar outcomes but 3rd class was very different. That seems plausible to me, so I'll try that approach (at the cost of slightly increasing the model complexity and overfitting risk).

#### Name
We wouldn't expect the name of the passenger itself to predict their survival. But there are a couple of things here we could usefully extract. Firstly, it might come in handy later on when we're trying to group passengers travelling together. Secondly, the name includes a title (e.g. Master, Miss, Sir, Dr, Mrs...). These might be useful when we're trying to impute missing data such as age.

#### Sex/Gender
Clearly a very important variable to include in the model, and since it's only two levels we don't need to manipulate it.

#### Age
Also likely to be very important, but finding an appropriate (probably nonlinear) mapping between age and survival might be the crucial task here. It may be worth trying to transform the number in a way that de-emphasises differences at the higher end of the scale, i.e. make 5 more different from 20 than 20 is from 35. There are also some missing values here - one simple approach would be to replace them with the mean, but we can do slightly better by using the title from the "Name" feature to give us a clue to whether the passenger is a child or an adult.

#### SibSp
This variable encodes the number of siblings and/or spouse travelling with the passenger. It's an overloaded variable, so we'll probably want to use the passenger age to tease apart spouse/sibling relationships. It may have predictive power on its own, but we can probably isolate a couple of useful things from this: The size of the family this person traveled with - a single man might react differently to the sinking than a childless husband or a father for example - and the survival (if known) of other members of their travelling party.

#### Parch
A similar overloaded variable encoding the number of parents and/or children travelling with the passenger. We'll try to combine this with the SibSp variable to find and relate families.

#### Ticket
A code with apparently little unique and useful information in itself. I think the best use of this information will be to find passengers travelling together, since their survival chances are probably linked. There's also apparently a different ticket format for those travelling as White Star Line staff (but not crew) - but on the whole I doubt that will buy us enough extra to be worth including.

#### Fare
Many models - including two of the benchmarks - use the fare as a predictor, but I'm skeptical. It seems to be a function of class and group size (since the listed ticket price covers everyone travelling under that ticket). Since we'll have both of those variables in our model anyway, there's not too much extra it can buy us. At a push, perhaps it will give us an extra clue about where in the ship someone was travelling - we can investigate that later.

#### Cabin
This seems like it should be a very useful variable, as it gives us some sense of where, physically, a passenger was on the ship. It's limited to just 23% of the passengers in the training set, but for those passengers we can extract z-axis (deck letter), x-axis (port or starboard, denoted by odd/even numbers) and y-axis (size of number) information. This could be relevant since the ship was struck on the starboard side, listed unevenly, and also sank bow first. 

A check of the [Titanic deck plans](http://www.encyclopedia-titanica.org/titanic-deckplans/) allows us to map out the position of each cabin precisely. Even if we just use the intuition above (port/starboard is odd/even, deck is letter, large/small numbers are at different ends of the ship) it turns out that's pretty accurate for the most part.

One major limitation to the cabin information is that while it's likely to be most useful for 3rd class passengers - those near where the iceberg struck are likely to have found it harder to escape as the passenger areas flooded - the majority of cabin records are for upper class passengers. Still, we'll try including those 3 directional axes as factors and perhaps even try an imputation for the missing data based on the location of cabins of each class.

#### Embarked
I'm choosing to ditch this data. It may have some relationship with location on the ship, if cabins were unassigned before boarding, but that's pretty speculative. I'm not convinced this variable is going to add enough real (and unique) information to justify the added complexity.

#### Overall
We're looking, then, at the following predictors:
- Age
- Sex
- Class (2 dummy variables)
- Family relationship
- Location

The latter two variables will be a little more complex and take some care to set up. For this stage of the tutorial, let's use just the first 3 predictors of age/sex/class and get a sense of how well different machine learning methods do with this relatively simple set of data. The idea will be to get a practical understanding not just of how the different approaches compare, but how the choice of meta-parameters (e.g. the number of hidden nodes in the neural network) affect their out-of-set classification accuracy.

## Part 3: Preparing the data

First, load up the libraries we'll be using:

```{r}
setwd("C:/Users/Iain/Desktop/R/Titanic")

suppressWarnings(suppressMessages(library(grid))) # For plotting 
suppressWarnings(suppressMessages(library(gridExtra))) # For plotting 
suppressWarnings(suppressMessages(library(neuralnet))) # Neural Network Models
suppressWarnings(suppressMessages(library(nnet))) # Neural Network Models
suppressWarnings(suppressMessages(library(plyr))) # Working with tidy data
suppressWarnings(suppressMessages(library(dplyr))) # Working with tidy data
suppressWarnings(suppressMessages(library(reshape2))) # Reshaping tables
suppressWarnings(suppressMessages(library(scales))) # For plotting 
suppressWarnings(suppressMessages(library(stringr))) # For manipulating strings
suppressWarnings(suppressMessages(library(adabag))) # For boosting 
suppressWarnings(suppressMessages(library(gbm))) # For boosting 
suppressWarnings(suppressMessages(library(randomForest))) # For boosting 
suppressWarnings(suppressMessages(library(caret))) # For training/selecting models 
```

Next, load up the data itself and organise it into a dplyr data table:

```{r}
rm(list = setdiff(ls(),c("trainraw","testraw")))

if (!exists("testraw")){
    print("Reading in data... just a moment")
    testraw <- read.csv("test.csv",stringsAsFactors=FALSE)
    print("Finished reading data.")
}

if (!exists("trainraw")){
    print("Reading in data... just a moment")
    trainraw <- read.csv("train.csv",stringsAsFactors=FALSE)
    print("Finished reading data.")
}

train <- tbl_df(trainraw)
n <- nrow(train)
train$Set <- "train"
test <- tbl_df(testraw)
test$Set <- "test"
test$Survived <- rep(2,nrow(test))
full <- rbind(test,train)

```

Do a bit of simple data cleaning, such as imputing missing ages:

```{r}
# Get title, ticket #, surname, deck, cabin #:
full$Title <- gsub(".*, ","",full$Name)
full$Title <- gsub("\\..*","",full$Title)
full$Ticket <- factor(full$Ticket,levels=sort(unique(full$Ticket)))
full$Surname <- gsub(",.*","",full$Name)
full$Deck <- as.factor(substr(gsub("[0-9].*","",full$Cabin),1,1))
#full$Cabinnum <- as.numeric(gsub("[^0-9]","",gsub(" .*","",full$Cabin)))

# Infer Age from title for NAs
# First group honorifics together, and combine languages (Mlle=Miss for example):
full$Title <- gsub("Mlle","Miss",full$Title)
full$Title <- gsub("Mme","Mrs",full$Title)
full$Title <- gsub("Ms","Mrs",full$Title)
full$Title <- gsub("Dona","Hon",full$Title)
full$Title <- gsub("Lady","Hon",full$Title)
full$Title <- gsub("the Countess","Hon",full$Title)
full$Title <- gsub("Col","Hon",full$Title)
full$Title <- gsub("Dr","Hon",full$Title)
full$Title <- gsub("Don","Hon",full$Title)
full$Title <- gsub("Rev","Hon",full$Title)
full$Title <- gsub("Major","Hon",full$Title)
full$Title <- gsub("Capt","Hon",full$Title)
full$Title <- gsub("Jonkheer","Hon",full$Title)
full$Title <- gsub("Sir","Hon",full$Title)
full$Title[full$Title=="Mr"&!full$SibSp==1] <- "Bach"

# Get mean ages for each group
grouped_data <- group_by(full[!is.na(full$Age),c("Title","Age")],Title)
nums <- table(grouped_data$Title)
ages <- summarise_each((grouped_data),funs(median))
ages$count <- nums

# Impute missing ages based on median for given title
title_age <- ages$Age[as.factor(full$Title)]
full$Age[is.na(full$Age)] <- title_age[is.na(full$Age)]
```

Finally, we'll build smaller versions of the dataset, using only the simpler features we've decided to include at this stage. We'll also normalise the data.

```{r, cache=TRUE}
# Very basic logistic model using Sex, Age, Class
# full$std_Age <- as.numeric(scale(log(full$Age)))
# #full$std_Age <- as.numeric(plogis((full$Age-18)/2))
# full$Sex <- as.factor(full$Sex)
# full$Pclass <- as.factor(full$Pclass)
# full$AgeFactor <- as.factor(full$Age>=18)
# 
# logit1 <- glm(Survived ~ Pclass + Sex*std_Age, data = full[full$Set=="train",], family = "binomial")

# Very basic neural network using Sex, Age, Class

y <- full$Survived[full$Set=="train"]   # 0 = Died, +1 = Survived
age <- as.numeric(full$Age[full$Set=="train"])/40-1   # Age scaled from ~ -1 to +1
classes <- full$Pclass[full$Set=="train"]
fclass <- -1+2*as.numeric(classes==1)   # 1 for First
sclass <- -1+2*as.numeric(classes==2)   # 1 for Second
sex <- as.integer(as.factor(full$Sex[full$Set=="train"]))*2-3   # -1 female, +1 male
nnetdata <- data.frame(age=age,sex=sex,fclass=fclass,sclass=sclass,y=y)

test_y <- full$Survived[full$Set=="test"]   # 0 = Died, +1 = Survived
test_age <- as.numeric(full$Age[full$Set=="test"])/40-1   # Age scaled from ~ -1 to +1
test_classes <- full$Pclass[full$Set=="test"]
test_fclass <- -1+2*as.numeric(test_classes==1)   # 1 for First
test_sclass <- -1+2*as.numeric(test_classes==2)   # 1 for Second
test_sex <- as.integer(as.factor(full$Sex[full$Set=="test"]))*2-3   # -1 female, +1 male
test_nnetdata <- data.frame(age=test_age,sex=test_sex,fclass=test_fclass,sclass=test_sclass,y=test_y)

x_age <- rep(seq(1,80,1),6)/40-1
x_fclass <- rep(c(rep(1,80),rep(-1,160)),2)
x_sclass <- rep(c(rep(-1,80),rep(1,80),rep(-1,80)),2)
x_sex <- c(rep(-1,240),rep(1,240))
x <- data.frame(age=x_age,sex=x_sex,fclass=x_fclass,sclass=x_sclass)
x$y <- numeric(nrow(x))
```

## Part 4: Model Fitting

First, let's set up the training control parameters for caret. We'll use repeated cross-validation on the training data:

```{r}

# Test which age/class/sex fit is best using cross-validation:

nnetdata$y[nnetdata$y==0]<-"Died"
nnetdata$y[nnetdata$y==1]<-"Survived"
nnetdata$y<-factor(nnetdata$y)

tc<-trainControl(method="repeatedcv",
                 number=10,
                 repeats=5,
                 classProbs=TRUE,
                 summaryFunction=twoClassSummary)

tc_acc<-trainControl(method="repeatedcv",
                 number=10,
                 repeats=5,
                 classProbs=FALSE)

```

Now we can use caret to examine the cross-validated performance of different models across a range of meta-parameters. We'll measure performance usign the receiver-operator characteristic (which takes into account the 'confidence' of a model's choice).

Why do we want to do this? Well first off, it's informative - we can get clues about how the models are fitting this particular dataset (and its properties), as well as getting a sense how different versions of these models vary in predictive power.

But another reason we might take this approach - rather than, say, automating the procedure entirely and just selecting the best combination of meta-parameters to fit the test data with - is that we can potentially avoid some of the overfitting that could introduce. Instead of automatically picking the meta-parameters for the model that happen to perform best on the data we give it, we can check how performance varies across different settings and choose some that sit in a good "range".

Let's now take a look at a few models we could apply to our data:

### Logistic Regression

```{r, cache=TRUE}

# Simple Logistic Regression
ptm <- proc.time()
mod_log<-train(y~.,data=nnetdata,
              method="glm",
              family="binomial",
              trControl=tc,
              metric="ROC")
proc.time() - ptm
mod_log
```

A simple logistic regression gives us a nice baseline. No meta-parameters to adjust.

### Neural Net

```{r, cache=TRUE}

# Neural Network
nnGrid<-expand.grid(size=seq(2,5,1),decay=10^seq(-5,-0.5,0.5))
ptm <- proc.time()
mod_nn<-train(y~.,
                data=nnetdata,
                method="nnet",
                trControl=tc,
                tuneGrid=nnGrid,
                metric="ROC",
                trace=FALSE)
proc.time() - ptm
mod_nn
ggplot(mod_nn) + coord_trans(x = "log10")

```

The neural network performs pretty solidly across a range of values. Generally, smaller numbers of hidden units seem to be better, which makes sense with our limited feature set, but 3 units seems to be the useful minimum. This could change as we add more complex (interacting) features. A little weight decay (but not too much) seems to help, too, though performance overall is fairly robust to our choice of parameter here.

### Random Forest

```{r, cache=TRUE}
# Random Forest
rfGrid<-data.frame(mtry=seq(1,10,1))
ptm <- proc.time()
mod_rf<-train(y~.,data=nnetdata,
              method="rf",
              trControl=tc,
              tuneGrid=rfGrid,
              metric="ROC",
              trace=FALSE)
proc.time() - ptm
mod_rf
ggplot(mod_rf)
```

[Random forests](https://en.wikipedia.org/wiki/Random_forest) are hugely popular in machine learning, especially in contests like Kaggle. Partly I suspect that's because they're conceptually easy to understand, and tend to give very strong fits to the training data. I'm a little cautious of them myself, since they tend to overfit a little too often for my liking, and they're almost impossible to interpret. The scientist in me wants at least a bit of explanatory power from my models - often there's a simpler truth under all the noisy data and I'm not sure forests always add much predictive power over other methods in such situations. But they're certainly a strong tool to have.

A quick glance of the random forest performances suggest we want our trees to select 2-3 predictors, which is roughly where we'd expect for classification (we have n=4 predictors, and a priori you would sample around n^0.5 for each tree).

### Bagged Trees

```{r, cache=TRUE}

# Bagged Trees
ptm <- proc.time()
mod_bag<-train(y~.,data=nnetdata,
              method="treebag",
              trControl=tc,
              tuneLength=2,
              metric="ROC",
              verbose=FALSE)
proc.time() - ptm
mod_bag
```

An [even simpler approach](https://en.wikipedia.org/wiki/Decision_tree_learning#Types) - but one more prone to overfitting - is to allow each tree to use all the predictors before combining them.

### Gradient Boosted Trees

```{r, cache=TRUE}

# Gradient Boosted Trees
gbmGrid<-expand.grid(interaction.depth=seq(1,5,1),n.trees=seq(50,250,50),shrinkage=seq(0.05,0.2,0.05))
ptm <- proc.time()
mod_gbm<-train(y~.,data=nnetdata,
              method="gbm",
              trControl=tc,
              tuneGrid=gbmGrid,
              metric="ROC",
              verbose=FALSE)
proc.time() - ptm
mod_gbm
ggplot(mod_gbm)
```

[Gradient boosted trees](https://en.wikipedia.org/wiki/Gradient_boosting) do best with depth 2 or more, and lower shrinkage of 0.05-0.10. Adding iterations above 100 doesn't look like it's worth it, and may even be detrimental at higher shrinkage rates. 

### Support Vector Machine

```{r, cache=TRUE}

# Support Vector Machine
svmGrid<-expand.grid(C=2^seq(-7,-3,1),sigma=2^seq(-3,3,1))
ptm <- proc.time()
mod_svm<-train(y~.,data=nnetdata,
              method="svmRadial",
              trControl=tc,
              tuneGrid=svmGrid,
              metric="ROC",
              verbose=FALSE)
proc.time() - ptm
mod_svm
ggplot(mod_svm) + coord_trans(x = "log10")
```

Using a [radial support vector machine](https://en.wikipedia.org/wiki/Support_vector_machine#Nonlinear_classification) (under the assumption that we want a smooth classifier, rather than a 'jagged' one) we get decent results for lower cost functions, and values of sigma around 0.5. Interestingly, though, we also get decent results using a much larger kernel, with sigma around 8, and an automated script would select this as the 'best' option. That might not make so much sense in general though, since 8 is an order of magnitude greater than the variance in our normalised feature set.

### Discriminant Analysis

```{r, cache=TRUE}

# Regularized Discriminant Analysis (very slow)
rdaGrid<-expand.grid(gamma=seq(0,0.08,0.02),lambda=seq(0.5,2.5,0.5))
ptm <- proc.time()
mod_rda<-train(y~.,data=nnetdata,
              method="rda",
              trControl=tc,
              tuneGrid=rdaGrid,
              metric="ROC",
              verbose=FALSE)
proc.time() - ptm
mod_rda
ggplot(mod_rda)
```

[Regularized discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) generally gives us similar power to the SVM and is pretty robust to the choice of gamma & lambda. It's sensitive to deviations from normality in the input data though (such deviations are likely, for example, with the passenger class features) and tends to take a longer time to run than most of our other options.

### Adaboost

```{r, cache=TRUE}

# Adaboost (slow)
adaGrid<-expand.grid(iter=seq(100,700,300),maxdepth=seq(2,5,1),nu=2^seq(-5,-1,1))
ptm <- proc.time()
mod_ada<-train(y~.,data=nnetdata,
              method="ada",
              trControl=tc,
              tuneGrid=adaGrid,
              metric="ROC",
              verbose=FALSE)
proc.time() - ptm
mod_ada
ggplot(mod_ada)
```

### K Nearest Neighbours

```{r, cache=TRUE}

# K Nearest Neighbours
ptm <- proc.time()
mod_knn<-train(y~.,data=nnetdata,
              method="kknn",
              trControl=tc_acc,
              tuneLength=2,
              verbose=FALSE)
proc.time() - ptm
mod_knn
```

K nearest neighbours is another simple and well-known algorithm that's useful for benchmarking the more complex ones.


```{r, cache=TRUE}

# Bagged flexible discriminant analysis
bfdGrid<-expand.grid(nprune=seq(5,20,5),degree=seq(1,3,1))
ptm <- proc.time()
mod_bfd<-train(y~.,data=nnetdata,
              method="bagFDA",
              trControl=tc,
              tuneGrid=bfdGrid,
              metric="ROC")
proc.time() - ptm
mod_bfd
ggplot(mod_bfd)
```

```{r, cache=TRUE}

resp_gbm<-predict(mod_gbm,nnetdata,type="prob")
resp_rf<-predict(mod_rf,nnetdata,type="prob")
resp_nn<-predict(mod_nn,nnetdata,type="prob")
resp_svm<-predict(mod_svm,nnetdata,type="prob")
resp_bfd<-predict(mod_bfd,nnetdata,type="prob")
resp_bag<-predict(mod_bag,nnetdata,type="prob")
resp_log<-predict(mod_log,nnetdata,type="prob")
resp_rda<-predict(mod_rda,nnetdata,type="prob")

raw_gbm<-predict(mod_gbm,nnetdata,type="raw")
raw_rf<-predict(mod_rf,nnetdata,type="raw")
raw_nn<-predict(mod_nn,nnetdata,type="raw")
raw_svm<-predict(mod_svm,nnetdata,type="raw")
raw_bfd<-predict(mod_bfd,nnetdata,type="raw")
raw_bag<-predict(mod_bag,nnetdata,type="raw")
raw_log<-predict(mod_log,nnetdata,type="raw")
raw_rda<-predict(mod_rda,nnetdata,type="raw")
raw_knn<-predict(mod_knn,nnetdata,type="raw")

probs<-data.frame(gbm=resp_gbm[,1],
                  rf=resp_rf[,1],
                  nn=resp_nn[,1],
                  svm=resp_svm[,1],
                  bfd=resp_bfd[,1],
                  bag=resp_bag[,1],
                  log=resp_log[,1],
                  rda=resp_rda[,1])

accs<-data.frame(gbm=sum(raw_gbm==nnetdata$y)/length(nnetdata$y),
                 rf=sum(raw_rf==nnetdata$y)/length(nnetdata$y),
                 nn=sum(raw_nn==nnetdata$y)/length(nnetdata$y),
                 svm=sum(raw_svm==nnetdata$y)/length(nnetdata$y),
                 bfd=sum(raw_bfd==nnetdata$y)/length(nnetdata$y),
                 bag=sum(raw_bag==nnetdata$y)/length(nnetdata$y),
                 log=sum(raw_log==nnetdata$y)/length(nnetdata$y),
                 rda=sum(raw_rda==nnetdata$y)/length(nnetdata$y),
                 knn=sum(raw_knn==nnetdata$y)/length(nnetdata$y))

corrplot(cor(probs))

resamps <- resamples(list(GBM = mod_gbm,
                          RF = mod_rf,
                          NN = mod_nn,
                          RDA = mod_rda,
                          BAG = mod_bag,
                          LOG = mod_log,
                          SVM = mod_svm,
                          BFD = mod_bfd,
                          KNN = mod_knn
                          ))

tr_pred_gbm <- predict(mod_gbm,nnetdata,type="prob")


```

Tune based on accuracies, rather than ROCs:

```{r}
# Neural Network
nnGrid<-expand.grid(size=seq(3,6,1),decay=10^seq(-4,-1,0.5))
ptm <- proc.time()
mod_nn<-train(y~.,
                data=nnetdata,
                method="nnet",
                trControl=tc_acc,
                tuneGrid=nnGrid,
                trace=FALSE)
proc.time() - ptm
```

```{r, cache=TRUE}
# Random Forest
rfGrid<-data.frame(mtry=seq(1,5,1))
mod_rf<-train(y~.,data=nnetdata,
              method="rf",
              trControl=tc_acc,
              tuneGrid=rfGrid,
              trace=FALSE)
proc.time() - ptm

```

```{r, cache=TRUE}
# Gradient Boosted Trees
gbmGrid<-expand.grid(interaction.depth=seq(2,4,1),n.trees=seq(25,150,25),shrinkage=seq(0.05,0.15,0.05))
mod_gbm<-train(y~.,data=nnetdata,
              method="gbm",
              trControl=tc_acc,
              tuneGrid=gbmGrid,
              verbose=FALSE)
proc.time() - ptm
```

```{r, cache=TRUE}
# Support Vector Machine
svmGrid<-expand.grid(C=2^seq(-6,-4,1),sigma=2^seq(-2,4,1))
mod_svm<-train(y~.,data=nnetdata,
              method="svmRadial",
              trControl=tc_acc,
              tuneGrid=svmGrid,
              verbose=FALSE)
proc.time() - ptm
```

```{r, cache=TRUE}

# Regularized Discriminant Analysis (very slow)
rdaGrid<-expand.grid(gamma=seq(0,0.08,0.02),lambda=seq(0,3,1))
mod_rda<-train(y~.,data=nnetdata,
              method="rda",
              trControl=tc_acc,
              tuneGrid=rdaGrid,
              verbose=FALSE)
proc.time() - ptm
```

```{r, cache=TRUE}

# Simple Logistic Regression
mod_log<-train(y~.,data=nnetdata,
              method="glm",
              family="binomial",
              trControl=tc_acc)
proc.time() - ptm
```

```{r, cache=TRUE}

# Bagged Trees
mod_bag<-train(y~.,data=nnetdata,
              method="treebag",
              trControl=tc_acc,
              verbose=FALSE)
proc.time() - ptm
```

```{r, cache=TRUE}

# K Nearest Neighbours
knnGrid<-expand.grid(kmax=seq(6,18,3),distance=seq(1,5,1),kernel="optimal")
mod_knn<-train(y~.,data=nnetdata,
              method="kknn",
              trControl=tc_acc,
              tuneGrid=knnGrid,
              verbose=FALSE)
proc.time() - ptm
```

```{r, cache=TRUE}

# Bagged flexible discriminant analysis
bfdGrid<-expand.grid(nprune=seq(3,12,3),degree=seq(1,4,1))
mod_bfd<-train(y~.,data=nnetdata,
              method="bagFDA",
              trControl=tc_acc,
              tuneGrid=bfdGrid)
proc.time() - ptm
```

```{r, cache=TRUE}


raw_gbm<-predict(mod_gbm,nnetdata,type="raw")
raw_rf<-predict(mod_rf,nnetdata,type="raw")
raw_nn<-predict(mod_nn,nnetdata,type="raw")
raw_svm<-predict(mod_svm,nnetdata,type="raw")
raw_bfd<-predict(mod_bfd,nnetdata,type="raw")
raw_rda<-predict(mod_rda,nnetdata,type="raw")
raw_bag<-predict(mod_bag,nnetdata,type="raw")
raw_log<-predict(mod_log,nnetdata,type="raw")
raw_knn<-predict(mod_knn,nnetdata,type="raw")

accs<-data.frame(gbm=sum(raw_gbm==nnetdata$y)/length(nnetdata$y),
                 rf=sum(raw_rf==nnetdata$y)/length(nnetdata$y),
                 nn=sum(raw_nn==nnetdata$y)/length(nnetdata$y),
                 svm=sum(raw_svm==nnetdata$y)/length(nnetdata$y),
                 bfd=sum(raw_bfd==nnetdata$y)/length(nnetdata$y),
                 rda=sum(raw_rda==nnetdata$y)/length(nnetdata$y),
                 bag=sum(raw_bag==nnetdata$y)/length(nnetdata$y),
                 log=sum(raw_log==nnetdata$y)/length(nnetdata$y),
                 knn=sum(raw_knn==nnetdata$y)/length(nnetdata$y))

prob_gbm<-predict(mod_gbm,nnetdata,type="prob")
prob_rf<-predict(mod_rf,nnetdata,type="prob")
prob_nn<-predict(mod_nn,nnetdata,type="prob")
prob_svm<-predict(mod_svm,nnetdata,type="prob")
prob_bfd<-predict(mod_bfd,nnetdata,type="prob")
prob_rda<-predict(mod_rda,nnetdata,type="prob")
prob_bag<-predict(mod_bag,nnetdata,type="prob")
prob_log<-predict(mod_log,nnetdata,type="prob")

# Stacked model
stackdata <- data.frame(gbm=prob_gbm[,1],
                  rf=prob_rf[,1],
                  nn=prob_nn[,1],
                  svm=prob_svm[,1],
                  bfd=prob_bfd[,1],
                  bag=prob_bag[,1],
                  log=prob_log[,1],
                  rda=prob_rda[,1],
                  knn=raw_knn,
                  y=nnetdata$y)
```

```{r, cache=TRUE}

nnGrid2<-expand.grid(size=seq(3,6,1),decay=10^seq(-3,0.5,0.5))
ptm<-proc.time()
mod_stack<-train(y~.,
                 data=stackdata,
                 method="nnet",
                 tuneGrid=nnGrid2,
                 trControl=tc_acc,
                 trace=FALSE)
proc.time() - ptm
mod_stack
ggplot(mod_stack) + coord_trans(x = "log10")


```

```{r}
# Predict from the stacked model:
stack_pred <- test[,1]

prob_gbm_test<-predict(mod_gbm,test_nnetdata,type="prob")
prob_rf_test<-predict(mod_rf,test_nnetdata,type="prob")
prob_nn_test<-predict(mod_nn,test_nnetdata,type="prob")
prob_svm_test<-predict(mod_svm,test_nnetdata,type="prob")
prob_bfd_test<-predict(mod_bfd,test_nnetdata,type="prob")
prob_rda_test<-predict(mod_rda,test_nnetdata,type="prob")
prob_bag_test<-predict(mod_bag,test_nnetdata,type="prob")
prob_log_test<-predict(mod_log,test_nnetdata,type="prob")
raw_knn_test<-predict(mod_knn,test_nnetdata,type="raw")

stackdata_test <- data.frame(gbm=prob_gbm_test[,1],
                  rf=prob_rf_test[,1],
                  nn=prob_nn_test[,1],
                  svm=prob_svm_test[,1],
                  bfd=prob_bfd_test[,1],
                  bag=prob_bag_test[,1],
                  log=prob_log_test[,1],
                  rda=prob_rda_test[,1],
                  knn=raw_knn_test,
                  y=test_nnetdata$y)

stack_pred$Survived <- as.numeric(predict(mod_stack, stackdata_test))-1
write.csv(stack_pred, file = "stack_pred.csv",row.names=FALSE)

gbm_pred<-stack_pred
gbm_pred$Survived <- as.numeric(predict(mod_gbm, test_nnetdata))-1
write.csv(gbm_pred, file = "gbm_pred.csv",row.names=FALSE)

svm_pred<-stack_pred
svm_pred$Survived <- as.numeric(predict(mod_svm, test_nnetdata))-1
write.csv(svm_pred, file = "svm_pred.csv",row.names=FALSE)

nn_pred<-stack_pred
nn_pred$Survived <- as.numeric(predict(mod_nn, test_nnetdata))-1
write.csv(nn_pred, file = "nn_pred.csv",row.names=FALSE)

rda_pred<-stack_pred
rda_pred$Survived <- as.numeric(predict(mod_rda, test_nnetdata))-1
write.csv(rda_pred, file = "rda_pred.csv",row.names=FALSE)

log_pred<-stack_pred
log_pred$Survived <- as.numeric(predict(mod_log, test_nnetdata))-1
write.csv(log_pred, file = "log_pred.csv",row.names=FALSE)

rf_pred<-stack_pred
rf_pred$Survived <- as.numeric(predict(mod_rf, test_nnetdata))-1
write.csv(rf_pred, file = "rf_pred.csv",row.names=FALSE)

bag_pred<-stack_pred
bag_pred$Survived <- as.numeric(predict(mod_bag, test_nnetdata))-1
write.csv(bag_pred, file = "bag_pred.csv",row.names=FALSE)

bfd_pred<-stack_pred
bfd_pred$Survived <- as.numeric(predict(mod_bfd, test_nnetdata))-1
write.csv(bfd_pred, file = "bfd_pred.csv",row.names=FALSE)

knn_pred<-stack_pred
knn_pred$Survived <- as.numeric(predict(mod_knn, test_nnetdata))-1
write.csv(knn_pred, file = "knn_pred.csv",row.names=FALSE)

```

Check how similar the predictions are from different models:

```{r}
raw_gbm_test<-as.numeric(predict(mod_gbm,test_nnetdata,type="raw"))-1
raw_rf_test<-as.numeric(predict(mod_rf,test_nnetdata,type="raw"))-1
raw_nn_test<-as.numeric(predict(mod_nn,test_nnetdata,type="raw"))-1
raw_svm_test<-as.numeric(predict(mod_svm,test_nnetdata,type="raw"))-1
raw_bfd_test<-as.numeric(predict(mod_bfd,test_nnetdata,type="raw"))-1
raw_rda_test<-as.numeric(predict(mod_rda,test_nnetdata,type="raw"))-1
raw_bag_test<-as.numeric(predict(mod_bag,test_nnetdata,type="raw"))-1
raw_log_test<-as.numeric(predict(mod_log,test_nnetdata,type="raw"))-1
raw_knn_test<-as.numeric(predict(mod_knn,test_nnetdata,type="raw"))-1
raw_stack_test<-as.numeric(stack_pred$Survived)

testpreds<-data.frame(gbm=raw_gbm_test,
                      rf=raw_rf_test,
                      bag=raw_bag_test,
                      bfd=raw_bfd_test,
                      rda=raw_rda_test,
                      svm=raw_svm_test,
                      nn=raw_nn_test,
                      log=raw_log_test,
                      knn=raw_knn_test,
                      stack=raw_stack_test)

plotdata<-data.frame(cmdscale(dist(t(as.matrix(testpreds)))))
minp<-min(plotdata)
maxp<-max(plotdata)
plotdata<-(plotdata-minp)/(maxp-minp)

ggplot(plotdata,aes(x=X2,y=X1,label=rownames(plotdata)))+
        geom_point() +
        geom_text() +
        coord_equal()
```



Taking a look at Decks and Cabin #s to see if there's anything in the passenger location that we can extract. We'll start by converting the cabin numbers to approximate locations, from -1 (stern) to 1 (bow). We'll use this [Titanic deckplan](http://www.encyclopedia-titanica.org/titanic-deckplans/).

```{r}
# Let's try to convert the cabin numbers to approximate locations:
full$x_loc <- numeric(nrow(full))
full$x_loc[full$Deck=="A"&!is.na(full$Cabinnum)




bydeck <- group_by(full[full$Set=="train"&!is.na(full$Cabinnum),c("Surv_corrected","Deck","Cabinnum")],Deck)

ggplot(data=bydeck,
       aes(x=Cabinnum, y=Surv_corrected)) +
       geom_point() +
       facet_grid(Deck ~ .) +
       geom_smooth(method = "lm", se=FALSE, color="blue", aes(group=1))
```

Not a lot there. Possibly there's a trend for better outcomes if you were near the front or back of the ship on a lower deck?

Let's also do our best to group families or people travelling on the same ticket.

```{r}
# Incorporate the corrected survival rates of anyone travelling on the same ticket
full$Surv_corrected[full$Set=="test"] <- 0
Tkt_Table <- group_by(full[,c("Surv_corrected","Ticket")],Ticket)
Tkt_Survival <- summarise_each(Tkt_Table,funs(sum))
full$Tkt_Survival <- Tkt_Survival$Surv_corrected[as.character(Tkt_Survival$Ticket)==as.character(full$Ticket)] 
```

